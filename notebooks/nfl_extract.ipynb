{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jThH6cacNpcl",
        "outputId": "de6a2f59-312f-4371-8b26-84c23c058391"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 32 team pages.\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/BUF → 14 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/MIA → 15 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/NE → 13 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/NYJ → 15 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/BAL → 12 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/CIN → 15 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/CLE → 14 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/PIT → 11 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/HOU → 16 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/IND → 15 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/JAX → 14 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/TEN → 14 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/DEN → 15 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/KC → 17 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/LV → 16 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/SD → 14 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/DAL → 12 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/NYG → 15 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/PHI → 16 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/WAS → 15 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/CHI → 15 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/DET → 15 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/GB → 14 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/MIN → 14 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/ATL → 16 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/CAR → 15 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/NO → 14 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/TB → 16 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/ARZ → 15 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/RAM → 16 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/SF → 17 records\n",
            "  https://www.ourlads.com/nfldepthcharts/depthchart/SEA → 15 records\n",
            "Wrote 470 unique records to nfl_players_with_tiers_2025.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "import re\n",
        "\n",
        "BASE_URL = \"https://www.ourlads.com\"\n",
        "DEPTHCHARTS_INDEX = BASE_URL + \"/nfldepthcharts/depthcharts.aspx\"\n",
        "\n",
        "# Target positions (including variants for WR)\n",
        "TARGET_POS = {\"QB\", \"WR\", \"RB\", \"TE\", \"LWR\", \"RWR\", \"SWR\"}\n",
        "\n",
        "def get_team_links():\n",
        "    resp = requests.get(DEPTHCHARTS_INDEX, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "    resp.raise_for_status()\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "    links = []\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"]\n",
        "        if href.startswith(\"/nfldepthcharts/depthchart/\"):\n",
        "            full = BASE_URL + href\n",
        "            if full not in links:\n",
        "                links.append(full)\n",
        "    return links\n",
        "\n",
        "def parse_team_depthchart(team_url):\n",
        "    resp = requests.get(team_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "    resp.raise_for_status()\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "    # Derive team abbreviation\n",
        "    team_abbrev = None\n",
        "    depwrapper = soup.find(\"div\", id=\"ctl00_phContent_DepWrapper\")\n",
        "    if depwrapper:\n",
        "        for c in depwrapper.get(\"class\", []):\n",
        "            if c.startswith(\"dt-\"):\n",
        "                team_abbrev = c[3:]\n",
        "                break\n",
        "    if not team_abbrev:\n",
        "        team_abbrev = team_url.rstrip(\"/\").split(\"/\")[-1].upper()\n",
        "\n",
        "    records = []\n",
        "    # Locate the table\n",
        "    table = None\n",
        "    if depwrapper:\n",
        "        table = depwrapper.find(\"table\", class_=\"table-bordered\")\n",
        "    if table is None:\n",
        "        table = soup.find(\"table\", class_=\"table-bordered\")\n",
        "    if table is None:\n",
        "        return records\n",
        "\n",
        "    tbody = table.find(\"tbody\")\n",
        "    if tbody is None:\n",
        "        tbody = table.find(\"tbody\", id=\"ctl00_phContent_dcTBody\")\n",
        "    if tbody is None:\n",
        "        return records\n",
        "\n",
        "    for tr in tbody.find_all(\"tr\"):\n",
        "        tds = tr.find_all(\"td\")\n",
        "        if len(tds) < 2:\n",
        "            continue\n",
        "\n",
        "        pos_raw = tds[0].get_text(strip=True)\n",
        "        pos = pos_raw.strip()\n",
        "        if pos in (\"LWR\", \"RWR\", \"SWR\"):\n",
        "            pos_norm = \"WR\"\n",
        "        else:\n",
        "            pos_norm = pos\n",
        "\n",
        "        if pos_norm not in TARGET_POS:\n",
        "            continue\n",
        "\n",
        "        # Iterate over player slots:\n",
        "        # The “player slots” are at tds indices 2, 4, 6, 8, ... ; their corresponding “No.” are at 1, 3, 5, 7, ...\n",
        "        # We'll index pairs: (num_idx, player_idx) = (1,2), (3,4), (5,6), (7,8), ...\n",
        "        # Tier is determined by which “player position slot” (1st -> tier 1, 2nd -> tier 2, etc.)\n",
        "        tier = 1\n",
        "        for num_idx in range(1, len(tds), 2):\n",
        "            player_idx = num_idx + 1\n",
        "            if player_idx >= len(tds):\n",
        "                break\n",
        "            a = tds[player_idx].find(\"a\")\n",
        "            if a and a.get_text(strip=True):\n",
        "                player_text = a.get_text(strip=True)\n",
        "\n",
        "                # Remove trailing depth chart key like \"SF24\", \"CF22\", \"SF21\", etc.\n",
        "                player_text = re.sub(\n",
        "                  r\"\\s+(?:[A-Z]{2}\\d{2}|\\d{2}/\\d|[A-Z]{1,2}/[A-Za-z]{2,3})$\",\n",
        "                  \"\",\n",
        "                  player_text,\n",
        "                  flags=re.IGNORECASE,\n",
        "                )\n",
        "\n",
        "                # Normalize capitalization (proper case)\n",
        "                player_text = player_text.title()\n",
        "\n",
        "                player_clean = player_text.strip()\n",
        "\n",
        "                records.append((player_clean, team_abbrev, pos_norm, tier))\n",
        "\n",
        "\n",
        "            tier += 1\n",
        "\n",
        "    return records\n",
        "\n",
        "def scrape_all(save_csv_path=\"nfl_players_with_tiers_2025.csv\"):\n",
        "    team_links = get_team_links()\n",
        "    print(f\"Found {len(team_links)} team pages.\")\n",
        "    all_records = []\n",
        "    for link in team_links:\n",
        "        try:\n",
        "            recs = parse_team_depthchart(link)\n",
        "            print(f\"  {link} → {len(recs)} records\")\n",
        "            all_records.extend(recs)\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing {link}: {e}\")\n",
        "        time.sleep(1)\n",
        "\n",
        "    # Deduplicate (optionally)\n",
        "    seen = set()\n",
        "    deduped = []\n",
        "    for rec in all_records:\n",
        "        if rec not in seen:\n",
        "            seen.add(rec)\n",
        "            deduped.append(rec)\n",
        "\n",
        "    with open(save_csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Player\", \"Team\", \"Position\", \"Tier\"])\n",
        "        for rec in deduped:\n",
        "            writer.writerow(rec)\n",
        "    print(f\"Wrote {len(deduped)} unique records to {save_csv_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    scrape_all()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqLVgK-aOjj2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "project-ai-ftsy-football-sum-Wgve298a-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
